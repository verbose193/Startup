import cv2
import numpy as np
import seaborn as sns
from imutils import paths
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import sys, os
import pandas as pd
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D
from keras.losses import categorical_crossentropy
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.utils import np_utils
from PIL import Image
import scipy.misc
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

input_path='/content/drive/My Drive/Thesis/'

data_filenames = []
data_labels = []

for filename in os.listdir(input_path + 'COVID-19'):
        data_filenames.append(input_path + 'COVID-19/' + filename)
        data_labels.append(0)
        
        
print (len(data_filenames))
print (len(data_labels))

for filename in os.listdir(input_path + 'NORMAL'):
    data_filenames.append(input_path + 'NORMAL/' + filename)
    data_labels.append(1)

for filename in os.listdir(input_path + 'Viral Pneumonia'):
    data_filenames.append(input_path + 'Viral Pneumonia/' + filename)
    data_labels.append(2)
       
data = []
counter = 0
for img in data_filenames[:data_size]:
        image = cv2.imread(img)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (224, 224))
        data.append(image)
        
data = np.array(data)/255.0
labels = np.array(data_labels)
labels=np_utils.to_categorical(labels, num_classes=3)        
        
(x_train, x_test, y_train, y_test) = train_test_split(
    data,
    labels,
    test_size=0.20,
    stratify=labels,
    random_state=42
)


LR = 0.001
EPOCHS = 20
num_labels = 3
batch_size = 64
INP_SIZE = (224,224,3)

model = Sequential()
model.add(Conv2D(input_shape=(x_train.shape[1:]),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

#2nd convolution layer
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

#3rd convolution layer
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

model.add(Flatten())
model.add(Dense(units=4096,activation="relu"))
model.add(Dense(units=4096,activation="relu"))
model.add(Dense(3, activation='softmax'))

x_train = x_train.reshape(x_train.shape[0], 224, 224, 3)
x_test = x_test.reshape(x_test.shape[0], 224, 224, 3)


optim = Adam(lr = LR, decay = LR/EPOCHS)
model.compile(loss="categorical_crossentropy", optimizer=optim, metrics=["accuracy"])
model.fit(
            x_train, y_train,
            batch_size=16,
            validation_data = (x_test, y_test),
            epochs = EPOCHS
        )




